# -*- coding: utf-8 -*-
"""next_day_log_return.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNbbheODI8qeapvxg8IxcjXAurh5UM42
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from datetime import datetime

ticker = "AAPL"
df = yf.download(
    ticker,
    start="2010-01-01",
    end=None,
    auto_adjust=False
)

df.columns = [x[0] for x in df.columns]

df.reset_index(inplace=True)

import seaborn as sns
sns.lineplot(data=df, x="Date", y="Close")

import numpy as np

df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))
df.dropna(inplace=True)

sns.lineplot(data=df, x="Date", y="log_return")

from statsmodels.tsa.stattools import adfuller

# ADF test on log returns
result = adfuller(df['log_return'])

print("ADF Statistic:", result[0])
print("p-value:", result[1])
print("Used lags:", result[2])
print("Number of observations:", result[3])

print("\nCritical Values:")
for key, value in result[4].items():
    print(f"   {key}: {value}")

"""this is statioanry"""

from statsmodels.tsa.seasonal import seasonal_decompose

# Decompose price series
decomp_price = seasonal_decompose(
    df['log_return'],
    model='additive',
    period=252  # ~1 trading year
)

decomp_price.plot()
plt.show()

"""no proper trend exist, seasonality is mechanical and it  also not imapacting more,resid has 0 mean but has veriance"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

# ACF & PACF on log returns
fig, axes = plt.subplots(1, 2, figsize=(14, 4))

plot_acf(df['log_return'], lags=40, ax=axes[0])
axes[0].set_title("ACF of Log Returns")

plot_pacf(df['log_return'], lags=40, ax=axes[1], method='ywm')
axes[1].set_title("PACF of Log Returns")

plt.tight_layout()
plt.show()

"""ACF and PACF plots show no significant autocorrelation in daily log returns, indicating that ARIMA(0,0,0) is an appropriate baseline. This suggests that linear autoregressive models have limited predictive power for return forecasting"""



from pmdarima import auto_arima
model = auto_arima(df['log_return'])
model.summary()

series = df['log_return'].values
train_size = int(len(series) * 0.8)

train, test = series[:train_size], series[train_size:]



from statsmodels.tsa.arima.model import ARIMA
model = ARIMA(train, order=(1,0,0))
model_fit = model.fit()

predictions = model_fit.forecast(steps=len(test))

from sklearn.metrics import mean_squared_error
rmse = np.sqrt(mean_squared_error(test, predictions))
print("RMSE:", rmse)

"""The ARIMA baseline achieves an RMSE of 0.0165, which is comparable to the inherent volatility of daily log returns. This indicates limited linear predictability in the conditional mean and motivates the exploration of nonlinear ML models with richer feature representations"""

import pandas as pd

df_feat = df.copy()

# Lag features
for lag in [1, 5, 10, 21]:
    df_feat[f'lag_{lag}'] = df_feat['log_return'].shift(lag)

# Rolling statistics
df_feat['roll_mean_5'] = df_feat['log_return'].rolling(5).mean()
df_feat['roll_mean_21'] = df_feat['log_return'].rolling(21).mean()
df_feat['roll_std_5'] = df_feat['log_return'].rolling(5).std()
df_feat['roll_std_21'] = df_feat['log_return'].rolling(21).std()

# Target (next day return)
df_feat['target'] = df_feat['log_return'].shift(-1)

df_feat = df_feat.dropna()

df_feat

df_feat.set_index('Date', inplace=True)

# Features and target
X = df_feat.drop(columns=['target'])
y = df_feat['target']

split_idx = int(len(df_feat) * 0.8)

X_train = X.iloc[:split_idx]
X_test  = X.iloc[split_idx:]

y_train = y.iloc[:split_idx]
y_test  = y.iloc[split_idx:]

print(X_train.index.max(), X_test.index.min())

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

xgb_model = XGBRegressor(
    n_estimators=300,
    max_depth=3,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    random_state=42
)

xgb_model.fit(X_train, y_train)

xgb_preds = xgb_model.predict(X_test)

xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))
print("XGBoost RMSE:", xgb_rmse)

"""Despite introducing nonlinear features and XGBoost, the model underperformed the ARIMA baseline, reinforcing the difficulty of predicting daily stock returns and supporting the efficient market hypothesis."""

def create_sequences(series, window=20):
    X, y = [], []
    for i in range(len(series) - window):
        X.append(series[i:i+window])
        y.append(series[i+window])
    return np.array(X), np.array(y)

X , y = create_sequences(df['log_return'].values)

X = X.reshape(X.shape[0],X.shape[1],1)

df['log_return'].shape

X.shape

n = len(X)

X_train,X_test = X[:int(n*0.8)],X[int(n*0.8):]
y_train,y_test = y[:int(n*0.8)],y[int(n*0.8):]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense,Dropout

model = Sequential()
model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50,return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=1))

model.compile(optimizer='adam',loss='mean_squared_error')

model.fit(
    X_train,
    y_train,
    epochs=20,
    batch_size=32,
    shuffle=False,   # ðŸš¨ MUST be False
    validation_split=0.1
)
model.save("models/lstm_model.keras")

import numpy as np
from sklearn.metrics import mean_squared_error

# Predict on test set
lstm_preds = model.predict(X_test).flatten()

# RMSE
lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_preds))
print("LSTM RMSE:", lstm_rmse)

"""Across statistical, machine learning, and deep learning models, performance converges near the volatility level of daily returns. This suggests that the primary limitation lies in the intrinsic unpredictability of daily stock returns rather than model expressiveness

ARIMA performed as well as more complex models, suggesting that daily returns lack exploitable structure. Model selection should be driven by data properties, not model complexity
"""

print(model_fit.forecast(1)[0])

# Forecast next 5 daily log returns
forecast_5 = model_fit.forecast(steps=5)

print("Next 5 daily log-return forecasts:")
print(forecast_5)

